{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced: Text Processing in Matrices\n",
    "\n",
    "## Load Natural Language Toolkit for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk)\n",
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> punkt\n",
      "    Downloading package punkt to /home/jovyan/nltk_data...\n",
      "      Package punkt is already up-to-date!\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Enter 'd' for Download, then 'punkt', and then 'q' for quit\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text files into dictionary\n",
    "\n",
    "As a \"corpus\" we fetched some data from Wikipedia, based on currently\n",
    "trendy (2/18/2017) topics.  Each topic had multiple interpretations, some of which \n",
    "we suspected would \"intersect\" in interesting ways (e.g., Trump/Putin, Cloud/Google, \n",
    "Cloud/Climate).  Others had various interpretations (e.g., there are many types of \n",
    "Football).  See _Wikipedia.ipynb_ for the original download code.\n",
    "\n",
    "Selected topics (for which the top-10 matches were returned by Wikipedia) were:\n",
    "\n",
    " * Pennsylvania\n",
    " * Trump\n",
    " * Apple\n",
    " * Google\n",
    " * Farm\n",
    " * Climate\n",
    " * Cloud\n",
    " * Football\n",
    " * Government\n",
    " * Putin\n",
    "\n",
    "*docs* is a map from file --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Cloud computing.txt\n",
      "Loaded United States farm bill.txt\n",
      "Loaded Mediterranean climate.txt\n",
      "Loaded Oort cloud.txt\n",
      "Loaded Trump family.txt\n",
      "Loaded Eric Trump.txt\n",
      "Loaded Animal Farm.txt\n",
      "Loaded Government of Malaysia.txt\n",
      "Loaded Apple I.txt\n",
      "Loaded Wind farm.txt\n",
      "Loaded Alpine climate.txt\n",
      "Loaded Putin khuilo!.txt\n",
      "Loaded Trump University.txt\n",
      "Loaded Donald Trump.txt\n",
      "Loaded Apple.txt\n",
      "Loaded Legal affairs of Donald Trump.txt\n",
      "Loaded Climate.txt\n",
      "Loaded Google Account.txt\n",
      "Loaded Flag football.txt\n",
      "Loaded Farm.txt\n",
      "Loaded History of Pennsylvania.txt\n",
      "Loaded Google Talk.txt\n",
      "Loaded Apple Corps.txt\n",
      "Loaded Apple III.txt\n",
      "Loaded Football in England.txt\n",
      "Loaded Climate model.txt\n",
      "Loaded Arrest of Vladimir Putin viral video.txt\n",
      "Loaded Government of the United Kingdom.txt\n",
      "Loaded Putin Must Go.txt\n",
      "Loaded Apple Inc..txt\n",
      "Loaded Calumet Farm.txt\n",
      "Loaded Cumulus cloud.txt\n",
      "Loaded Arcus cloud.txt\n",
      "Loaded Vladimir Putin.txt\n",
      "Loaded Google Developers.txt\n",
      "Loaded Government in exile.txt\n",
      "Loaded Province of Pennsylvania.txt\n",
      "Loaded Forms of government.txt\n",
      "Loaded Foreign policy of Vladimir Putin.txt\n",
      "Loaded Mushroom cloud.txt\n",
      "Loaded Football player.txt\n",
      "Loaded E-government.txt\n",
      "Loaded State Farm Insurance.txt\n",
      "Loaded Head of government.txt\n",
      "Loaded Pennsylvania Dutch.txt\n",
      "Loaded Home Farm F.C..txt\n",
      "Loaded Outline of Pennsylvania.txt\n",
      "Loaded Google Books.txt\n",
      "Loaded Stratus cloud.txt\n",
      "Loaded Brook Farm.txt\n",
      "Loaded Pennsylvania.txt\n",
      "Loaded Pennsylvania Historical and Museum Commission.txt\n",
      "Loaded Google Hangouts.txt\n",
      "Loaded College football.txt\n",
      "Loaded Google Videos.txt\n",
      "Loaded Geography of Pennsylvania.txt\n",
      "Loaded Pennsylvania Regions.txt\n",
      "Loaded Google Search.txt\n",
      "Loaded Tag cloud.txt\n",
      "Loaded Apple TV.txt\n",
      "Loaded Climate classification.txt\n",
      "Loaded Football team.txt\n",
      "Loaded Google.txt\n",
      "Loaded Government.txt\n",
      "Loaded Melania Trump.txt\n",
      "Loaded The Trump Organization.txt\n",
      "Loaded Public image of Vladimir Putin.txt\n",
      "Loaded Township (Pennsylvania).txt\n",
      "Loaded AtGoogleTalks.txt\n",
      "Loaded Climate justice.txt\n",
      "Loaded Trump fragrances.txt\n",
      "Loaded .google.txt\n",
      "Loaded Putin. War.txt\n",
      "Loaded Government of Australia.txt\n",
      "Loaded Crimean speech of Vladimir Putin.txt\n",
      "Loaded Happy Birthday, Mr. Putin!.txt\n",
      "Loaded Russia under Vladimir Putin.txt\n",
      "Loaded Australian rules football.txt\n",
      "Loaded Google+.txt\n",
      "Loaded HP Cloud.txt\n",
      "Loaded Pennsylvania Railroad.txt\n",
      "Loaded Cooking apple.txt\n",
      "Loaded Local government.txt\n",
      "Loaded American football.txt\n",
      "Loaded Association football.txt\n",
      "Loaded CLOUD experiment.txt\n",
      "Loaded Government agency.txt\n",
      "Loaded Apple II series.txt\n",
      "Loaded Apple Store.txt\n",
      "Loaded Cloud.txt\n",
      "Loaded Family of Donald Trump.txt\n",
      "Loaded Climate change.txt\n",
      "Loaded Football.txt\n",
      "Loaded Century Farm.txt\n",
      "Loaded Farm Aid.txt\n",
      "Loaded Pro Football Hall of Fame.txt\n",
      "Loaded Desert climate.txt\n",
      "Loaded Oceanic climate.txt\n",
      "Loaded Subarctic climate.txt\n",
      "All files loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "docs = {}\n",
    "\n",
    "for filename in os.listdir('text'):\n",
    "    if filename == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    file = open('text/' + filename)\n",
    "    docs[filename] = file.read()\n",
    "    print ('Loaded',filename)\n",
    "\n",
    "print (\"All files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preliminaries to get you started.\n",
    "\n",
    "The function *has_letter* should be used to filter words based on the presence of a letter.\n",
    "\n",
    "The set *stopwords* includes words to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "# Returns True if the input (string) parameter has\n",
    "# any sort of letter in it, else returns False.\n",
    "\"\"\"\n",
    "def has_letter(x):\n",
    "    return re.match('.*[a-zA-Z].*',x) != None\n",
    "\n",
    "# Stopwords are words we will ignore for search\n",
    "# purposes, because they are too common to be useful\n",
    "stopwords = set()\n",
    "\n",
    "stop_file = open('stopwords.txt')\n",
    "for line in stop_file:\n",
    "    stopwords.add(line.strip())\n",
    "\n",
    "# The NLTK parser breaks apostrophe-s into a separate \"word\"\n",
    "# so we'll want to add it to the list... Though it's technically\n",
    "# not a stop word in the traditional sense.\n",
    "stopwords.add(\"'s\")\n",
    "\n",
    "# Use this as the maximum number of words we will index\n",
    "MAX_WORDS = 18102\n",
    "\n",
    "# Create the word stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Code Goes Here!\n",
    "\n",
    "Note that you may want to read more about TF*IDF scoring at:\n",
    "\n",
    "* http://nlp.stanford.edu/IR-book/html/htmledition/term-frequency-and-weighting-1.html\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "################         Create the doc_vector function   #############\n",
    "#######################################################################\n",
    "\n",
    "def doc_vector(content, vector, lexicon, inverse_lexicon, stopwords, word_count):\n",
    "    test = nltk.word_tokenize(content)\n",
    "    #test = [stemmer.stem(t) for t in test]\n",
    "    for word in test:\n",
    "        \n",
    "        word = word.lower()\n",
    "        \n",
    "        if has_letter(word):\n",
    "            if word not in stopwords:\n",
    "\n",
    "                word = stemmer.stem(word) \n",
    "                \n",
    "                if word in lexicon.keys():\n",
    "                    vector[lexicon[word]] += 1\n",
    "                    continue\n",
    "                \n",
    "                if word_count >= MAX_WORDS:\n",
    "                    continue\n",
    "                    \n",
    "                #add to lexicon here\n",
    "                lexicon[word] = word_count\n",
    "                inv_lexicon[word_count] = word\n",
    "                vector[word_count] += 1\n",
    "                word_count += 1\n",
    "                \n",
    "                \n",
    "        #here you create word frequency\n",
    "        #if word in lexicon.keys():\n",
    "         #   vector[lexicon[word]] += 1\n",
    "          #  corpus[doc_index][lexicon[word]] = 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6946052 ,  0.61542395,  0.3723859 , ...,  1.99563519,\n",
       "        1.99563519,  1.99563519])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################\n",
    "################           Call the doc_vector function   #############\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "matrix = np.zeros((len(docs),MAX_WORDS))\n",
    "\n",
    "lexicon = {}\n",
    "inv_lexicon = {}\n",
    "\n",
    "word_count = 0\n",
    "doc_index = 0\n",
    "\n",
    "for doc in docs:\n",
    "    word_count = doc_vector(docs[doc], matrix[doc_index,:], lexicon, inv_lexicon, stopwords, word_count)\n",
    "    #print(matrix[doc_index])\n",
    "    doc_index += 1\n",
    "    \n",
    "idf = np.log10(len(docs)/np.count_nonzero(matrix,axis=0))\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_vector(query):\n",
    "    bs = 0\n",
    "    return_vector = np.zeros(MAX_WORDS)\n",
    "    #return_vector, bs = doc_vector(query, return_vector, lexicon, inv_lexicon, stopwords, index)\n",
    "    doc_vector(query, return_vector, lexicon, inv_lexicon, stopwords, word_count)\n",
    "    return return_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        new_matrix = np.multiply(vectors,idf)\\n        new_query = np.multiply(query,idf)\\n        result = np.dot(new_matrix,new_query)/(norm(new_matrix)*norm(new_query))\\n\\n        result_df = pd.DataFrame(result).reset_index()\\n        result_df.columns = [\"docid\", \"score\"]\\n        names_df = pd.DataFrame(doc_name).reset_index()\\n        names_df.columns = [\"docid\", \"docname\"]\\n\\n        final_df = result_df.merge(names_df, left_on=[\\'docid\\'], right_on=[\\'docid\\'])\\n        final_df = final_df[[\\'docid\\',\\'docname\\',\\'score\\']]\\n\\n        final_df = final_df.sort_values(by=\\'score\\', ascending=False)\\n        final_df = final_df[:num_results]\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\n",
    "#doc_name = []\n",
    "#for doc in docs:\n",
    "#    doc_name.append(doc)\n",
    "    \n",
    "    \n",
    "def search(vectors, idf, query, num_results):\n",
    "    doc_name = list(docs.keys())\n",
    "    \n",
    "    final_df = pd.DataFrame(columns=['docid','docname','score'])\n",
    "    \n",
    "    for row in range(vectors.shape[0]):\n",
    "        new_matrix = vectors[row,:]*idf\n",
    "        new_query = query*idf\n",
    "        result = np.dot(new_matrix,new_query)/(norm(new_matrix)*norm(new_query))\n",
    "\n",
    "        final_df.loc[row] = [int(row),doc_name[row],result]\n",
    "        \n",
    "        #result_df.columns = [\"docid\", \"score\"]\n",
    "        #names_df = pd.DataFrame(doc_name).reset_index()\n",
    "        #names_df.columns = [\"docid\", \"docname\"]\n",
    "\n",
    "        #final_df = result_df.merge(names_df, left_on=['docid'], right_on=['docid'])\n",
    "        #final_df = final_df[['docid','docname','score']]\n",
    "\n",
    "        final_df = final_df.sort_values(by='score', ascending=False)\n",
    "        final_df = final_df[:num_results]\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "        new_matrix = np.multiply(vectors,idf)\n",
    "        new_query = np.multiply(query,idf)\n",
    "        result = np.dot(new_matrix,new_query)/(norm(new_matrix)*norm(new_query))\n",
    "\n",
    "        result_df = pd.DataFrame(result).reset_index()\n",
    "        result_df.columns = [\"docid\", \"score\"]\n",
    "        names_df = pd.DataFrame(doc_name).reset_index()\n",
    "        names_df.columns = [\"docid\", \"docname\"]\n",
    "\n",
    "        final_df = result_df.merge(names_df, left_on=['docid'], right_on=['docid'])\n",
    "        final_df = final_df[['docid','docname','score']]\n",
    "\n",
    "        final_df = final_df.sort_values(by='score', ascending=False)\n",
    "        final_df = final_df[:num_results]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docname</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Apple Inc..txt</td>\n",
       "      <td>0.490086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Apple I.txt</td>\n",
       "      <td>0.442409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Apple III.txt</td>\n",
       "      <td>0.401514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87.0</td>\n",
       "      <td>Apple II series.txt</td>\n",
       "      <td>0.349863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88.0</td>\n",
       "      <td>Apple Store.txt</td>\n",
       "      <td>0.342035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Apple.txt</td>\n",
       "      <td>0.330999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81.0</td>\n",
       "      <td>Cooking apple.txt</td>\n",
       "      <td>0.305086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59.0</td>\n",
       "      <td>Apple TV.txt</td>\n",
       "      <td>0.300357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Apple Corps.txt</td>\n",
       "      <td>0.276130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Home Farm F.C..txt</td>\n",
       "      <td>0.019465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid              docname     score\n",
       "29   29.0       Apple Inc..txt  0.490086\n",
       "8     8.0          Apple I.txt  0.442409\n",
       "23   23.0        Apple III.txt  0.401514\n",
       "87   87.0  Apple II series.txt  0.349863\n",
       "88   88.0      Apple Store.txt  0.342035\n",
       "14   14.0            Apple.txt  0.330999\n",
       "81   81.0    Cooking apple.txt  0.305086\n",
       "59   59.0         Apple TV.txt  0.300357\n",
       "22   22.0      Apple Corps.txt  0.276130\n",
       "45   45.0   Home Farm F.C..txt  0.019465"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_vector = create_query_vector('Apple Steve jobs')\n",
    "df = search(matrix,idf,search_vector,10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docname</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Donald Trump.txt</td>\n",
       "      <td>0.661491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Legal affairs of Donald Trump.txt</td>\n",
       "      <td>0.636261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65.0</td>\n",
       "      <td>The Trump Organization.txt</td>\n",
       "      <td>0.627995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>Trump University.txt</td>\n",
       "      <td>0.593636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66.0</td>\n",
       "      <td>Public image of Vladimir Putin.txt</td>\n",
       "      <td>0.591413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid                             docname     score\n",
       "13   13.0                    Donald Trump.txt  0.661491\n",
       "15   15.0   Legal affairs of Donald Trump.txt  0.636261\n",
       "65   65.0          The Trump Organization.txt  0.627995\n",
       "12   12.0                Trump University.txt  0.593636\n",
       "66   66.0  Public image of Vladimir Putin.txt  0.591413"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_vector = create_query_vector('Trump Putin')\n",
    "df = search(matrix,idf,search_vector,5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docname</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62.0</td>\n",
       "      <td>Google.txt</td>\n",
       "      <td>0.641453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Google Developers.txt</td>\n",
       "      <td>0.509939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>Google Account.txt</td>\n",
       "      <td>0.504314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Cloud computing.txt</td>\n",
       "      <td>0.494979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Google Talk.txt</td>\n",
       "      <td>0.473183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docid                docname     score\n",
       "62   62.0             Google.txt  0.641453\n",
       "34   34.0  Google Developers.txt  0.509939\n",
       "17   17.0     Google Account.txt  0.504314\n",
       "0     0.0    Cloud computing.txt  0.494979\n",
       "21   21.0        Google Talk.txt  0.473183"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_vector = create_query_vector('Google Cloud')\n",
    "df = search(matrix,idf,search_vector,5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
