{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting point for Spark on Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emrspark import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "conf.set(\"fs.s3n.awsAccessKeyId\",\"AKIAIESYJZVDMF75XC5Q\")\n",
    "conf.set(\"fs.s3n.awsSecretAccessKey\",\"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\")\n",
    "spark = SparkSession.builder.config(conf=conf).appName('Graph HW3').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading compressed image of sx-stackoverflow-a2q.txt\n",
      "Saved sx-stackoverflow-a2q.txt\n",
      "Downloading compressed image of sx-stackoverflow-c2q.txt\n",
      "Saved sx-stackoverflow-c2q.txt\n",
      "Downloading compressed image of sx-stackoverflow-c2a.txt\n",
      "Saved sx-stackoverflow-c2a.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: read files, load graph_sdf, etc.\n",
    "\n",
    "# Download and decompress data into your Jupyter environment\n",
    "import urllib.request\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "\n",
    "for file in ['sx-stackoverflow-a2q.txt','sx-stackoverflow-c2q.txt','sx-stackoverflow-c2a.txt']:\n",
    "    print ('Downloading compressed image of', file)\n",
    "    source = urllib.request.urlopen(\"https://snap.stanford.edu/data/\" + file + \".gz\")\n",
    "    compressedFile = io.BytesIO(source.read())\n",
    "    decompressedFile = gzip.GzipFile(fileobj=compressedFile)\n",
    "\n",
    "    with open(file, 'wb') as outfile:\n",
    "        outfile.write(decompressedFile.read())\n",
    "        outfile.close()\n",
    "        print ('Saved', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lines from the text file\n",
    "#answers_sdf = spark.read.load('sx-stackoverflow-a2q.txt', format=\"text\")\n",
    "#comments_answers_sdf = spark.read.load('sx-stackoverflow-c2a.txt', format=\"text\")\n",
    "#comments_questions_sdf = spark.read.load('sx-stackoverflow-c2q.txt', format=\"text\")\n",
    "\n",
    "answers_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ') \\\n",
    "         .load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-a2q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_answers_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ') \\\n",
    "         .load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-c2a.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_questions_sdf = spark.read.format(\"com.databricks.spark.csv\").option(\"delimiter\", ' ') \\\n",
    "         .load(\"s3n://upenn-bigdataanalytics/data/sx-stackoverflow-c2q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf = answers_sdf.unionAll(comments_questions_sdf)\n",
    "graph_sdf = graph_sdf.unionAll(comments_answers_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sdf = graph_sdf.select(graph_sdf._c0, graph_sdf._c1)\n",
    "graph_sdf = graph_sdf.withColumnRenamed('_c0', 'from_node')\n",
    "graph_sdf = graph_sdf.withColumnRenamed('_c1', 'to_node')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|from_node|to_node|\n",
      "+---------+-------+\n",
      "|        9|      8|\n",
      "|        1|      1|\n",
      "|       13|      1|\n",
      "|       17|      1|\n",
      "|       48|      2|\n",
      "|       17|      1|\n",
      "|       19|      9|\n",
      "|       13|     23|\n",
      "|       13|     11|\n",
      "|       23|     23|\n",
      "+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph_sdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|node|\n",
      "+----+\n",
      "|   3|\n",
      "|   5|\n",
      "|   1|\n",
      "|   4|\n",
      "|   2|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute a Spark DataFrame called nodes_sdf with all node IDs in the graph strictly less than 8\n",
    "# Compute nodes_sdf\n",
    "nodes_sdf = graph_sdf.select(graph_sdf['from_node']).filter(graph_sdf['from_node'] < 8).dropDuplicates()\n",
    "#nodes_sdf = nodes_sdf.select(nodes_sdf['from_node']).dropDuplicates()\n",
    "nodes_sdf = nodes_sdf.withColumnRenamed('from_node', 'node')\n",
    "nodes_sdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_is_empty(sdf):\n",
    "    try:\n",
    "        sdf.take(1)\n",
    "        return False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transitive_closure(G, origins, max_depth):\n",
    "    \n",
    "    depth = 0\n",
    "    #schema = StructType([StructField(\"node\", IntegerType(), True)])\n",
    "    #frontier_sdf = spark.createDataFrame(origins, schema)\n",
    "    frontier_sdf = origins\n",
    "    G = G.cache()\n",
    "    frontier_sdf = frontier_sdf.cache()\n",
    "     \n",
    "    for depth in range(max_depth-1):\n",
    "        reachable_sdf = frontier_sdf.alias('df1').join(G.alias('df2'), F.col('df1.node') == F.col('df2.from_node'))\n",
    "        reachable_sdf = reachable_sdf.select('to_node')\n",
    "        reachable_sdf = reachable_sdf.dropDuplicates()\n",
    "        \n",
    "        if sdf_is_empty(reachable_sdf) == False:\n",
    "            if depth == 0:           \n",
    "                visited_sdf = frontier_sdf\n",
    "                return_sdf = visited_sdf\n",
    "            #else:\n",
    "            #    visited_sdf = visited_sdf.unionAll(frontier_sdf)\n",
    "            #    return_sdf = return_sdf.unionAll(reached_sdf.withColumn('depth',F.lit(depth)))\n",
    "\n",
    "            #Here I remove both visited and frontier nodes from G\n",
    "            G = G.join(frontier_sdf, frontier_sdf['node']==G['to_node'], 'leftanti')\n",
    "            G = G.join(frontier_sdf, frontier_sdf['node']==G['from_node'], 'leftanti')\n",
    "\n",
    "            #Here I update frontier and reached for my next iteration\n",
    "            frontier_sdf = reachable_sdf\n",
    "            frontier_sdf = frontier_sdf.distinct().withColumnRenamed('to_node', 'node')\n",
    "            reached_sdf = reachable_sdf.join(visited_sdf, reachable_sdf['to_node'] == visited_sdf['node'], 'leftanti')\n",
    "\n",
    "            visited_sdf = visited_sdf.unionAll(frontier_sdf)\n",
    "            return_sdf = return_sdf.unionAll(reached_sdf)\n",
    "        if sdf_is_empty(reachable_sdf) == True:\n",
    "            break\n",
    "    \n",
    "    G = G.unpersist()\n",
    "    frontier_sdf = frontier_sdf.unpersist()\n",
    "    \n",
    "    return return_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_sdf = transitive_closure(graph_sdf, nodes_sdf, 3)\n",
    "#friend_recommendations_sdf.repartition(50, 'init_node')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246211"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reachable_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   node|\n",
      "+-------+\n",
      "|      3|\n",
      "|      5|\n",
      "|      1|\n",
      "|      4|\n",
      "|      2|\n",
      "| 232495|\n",
      "| 461561|\n",
      "|  80243|\n",
      "|  18333|\n",
      "|    296|\n",
      "|    691|\n",
      "|  13442|\n",
      "|1018842|\n",
      "| 311884|\n",
      "|  22436|\n",
      "|  23113|\n",
      "|  95869|\n",
      "|  49280|\n",
      "| 411813|\n",
      "|  34211|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reachable_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
